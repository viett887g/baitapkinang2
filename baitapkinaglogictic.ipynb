{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMo+3JDJE+4gNM/U0cfchIB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viett887g/baitapkinang2/blob/main/baitapkinaglogictic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing\n",
        "from scipy import optimize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "NZrCI3wqX0F_"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "xaPtkE7aL7xf"
      },
      "outputs": [],
      "source": [
        "def readData(filePath: str, filename: str):\n",
        "    data = np.loadtxt(os.path.join(filePath, filename), delimiter = ',')\n",
        "    data=preprocessing.MinMaxScaler().fit_transform(data)\n",
        "    #data=preprocessing.StandardScaler().fit_transform(data)\n",
        "    X = data[:,:-1]\n",
        "    y = data[:, -1]\n",
        "    m = X.shape[0]\n",
        "    n = X.shape[1]\n",
        "    X = np.reshape(X, (m,n))\n",
        "    y = np.reshape(y, (m,1))\n",
        "    #Them cot x0 = 1 vao X\n",
        "    x0 = np.ones((m,1))\n",
        "    X = np.column_stack([x0, X])\n",
        "    return X, y\n",
        "\n",
        "def featureVectorScaling(data):\n",
        "    avg = np.mean(data)\n",
        "    sln = data.max()\n",
        "    snn = data.min()\n",
        "    data_scl = (data - avg)/(sln - snn)\n",
        "    print(data_scl[1])\n",
        "    return data_scl\n",
        "\n",
        "def normalizeData(X):\n",
        "    X_scl = X[:, 0]\n",
        "    for i in range(1, X.shape[1]):\n",
        "        scl = featureVectorScaling(X[:, i])\n",
        "        X_scl = np.column_stack([X_scl, scl])\n",
        "    return X_scl\n",
        "\n",
        "#Day chinh la ham  hw(X)\n",
        "def sigmoid(X, w):\n",
        "    result = 1/(1 + np.exp(-np.dot(X, w)))\n",
        "    return result\n",
        "\n",
        "def loss(w, X, y):\n",
        "    m = X.shape[0]\n",
        "    h = sigmoid(X, w)\n",
        "    result = (-1 / m) * np.sum(np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h)))\n",
        "    return result\n",
        "\n",
        "def scipy(X,y,w,n_iters):\n",
        "    result = optimize.minimize(fun=loss, x0=w, args=(X,y),method='L-BFGS-B',options={\"maxiter\":n_iters} )\n",
        "    w_optimal = result.x\n",
        "    J_optimal = result.fun\n",
        "    return w_optimal, J_optimal\n",
        "\n",
        "def scipy2(X,y,w,n_iters):\n",
        "    result = optimize.minimize(fun=loss, x0=w, args=(X,y),method='Powell',options={\"maxiter\":n_iters} )\n",
        "    w_optimal2 = result.x\n",
        "    J_optimal2 = result.fun\n",
        "    return w_optimal2, J_optimal2  \n",
        "\n",
        "def gradient(X, y, w):\n",
        "    m = X.shape[0]\n",
        "    result = (1/m)*np.dot(X.T, sigmoid(X, w) - y)\n",
        "    return result\n",
        "\n",
        "def gradientDescent(X, y, w, alpha, n_iters):\n",
        "    w_optimal = w.copy()\n",
        "    J_history = []\n",
        "    for i in range(n_iters):\n",
        "        w_optimal = w_optimal - alpha*gradient(X, y, w_optimal)\n",
        "        J_history.append(loss(X, y, w_optimal))\n",
        "    return w_optimal, J_history\n",
        "\n",
        "#Hàm dự đoán nếu y_pred >=0.5 làm tròn thành 1, ngược lại là 0\n",
        "def predict(y_pred):\n",
        "    return np.rint(y_pred)\n",
        "\n",
        "def acc_score(y, y_hat):\n",
        "    result = accuracy_score(y, y_hat)\n",
        "    return  result\n",
        "\n",
        "\n",
        "def nggeg(X,y):\n",
        "  model = LogisticRegression().fit(X,y)\n",
        "  w_otp0 = model.coef_\n",
        "  return w_otp0\n",
        "\n",
        "def kfolk(X):\n",
        "  kf = KFold(n_splits=10)\n",
        "  kf.get_n_splits(X)\n",
        "  KFold(n_splits=10, random_state=None, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn import datasets, linear_model\n",
        ">>> from sklearn.model_selection import cross_validate\n",
        ">>> from sklearn.metrics import make_scorer\n",
        ">>> from sklearn.metrics import confusion_matrix\n",
        ">>> from sklearn.svm import LinearSVC"
      ],
      "metadata": {
        "id": "tyud7fK_dbB4"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crossvalidate(X,y):\n",
        "  diabetes = datasets.load_diabetes()\n",
        "  lasso = linear_model.Lasso()\n",
        "  cv_results = cross_validate(lasso, X, y, cv=4)\n",
        "  print(cv_results['test_score'])"
      ],
      "metadata": {
        "id": "9Kr2smC7dWNw"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn import datasets, linear_model\n",
        ">>> from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "WNFfh5ZEekgT"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_valscore(X,y):\n",
        "  diabetes = datasets.load_diabetes()\n",
        "  lasso = linear_model.Lasso()\n",
        "  print(cross_val_score(lasso, X, y))"
      ],
      "metadata": {
        "id": "UQRgklk9ejUD"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn import datasets, linear_model\n",
        ">>> from sklearn.model_selection import cross_val_predict"
      ],
      "metadata": {
        "id": "Dp2C4oZcf6Mx"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_valpredict1(X,y):\n",
        "  diabetes = datasets.load_diabetes()\n",
        "\n",
        "  cv = KFold(n_splits=4)\n",
        "  lasso = linear_model.Lasso()\n",
        "  y_pred = cross_val_predict(lasso, X, y, cv=cv)\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "nF5lIyWAjuSY"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    X, y = readData('/content','ex2data1.txt')\n",
        "    X = normalizeData(X)\n",
        "    n = X.shape[1]\n",
        "    w = np.zeros((n, 1))\n",
        "    alpha = 0.01\n",
        "    n_iters = 2000\n",
        "    #Chia train - test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.30,random_state=15)\n",
        "    w_optimal, J_optimal=scipy(X_train,y_train,w,n_iters)\n",
        "    #w_opt, J_hist = gradientDescent(X_train, y_train, w, alpha, n_iters)\n",
        "    w_optimal2, J_optimal2=scipy2(X_train,y_train,w,n_iters)\n",
        "    print('Trong so w toi uu la L-BFGS-B: ', w_optimal)\n",
        "    print('tGia tri Loss toi uu: ', J_optimal)\n",
        "    print('tTrong so w toi uu la Powell: ', w_optimal2)\n",
        "    print('tGia tri Loss toi uu: ', J_optimal2)\n",
        "    y_hat = predict(sigmoid(X_test,w_optimal2))\n",
        "    \n",
        "    print('Sử dụng sklearn, Acc: ',acc_score(y_test, y_hat))\n",
        "    print(\"....\\nmô hình LogisticRegression\")\n",
        "    w_otp0=nggeg(X_train,y_train)\n",
        "    print(\"trong so toi uu \",w_otp0)\n",
        "    y_hat4=predict(sigmoid(X_test,w_otp0[0]))\n",
        "    print('Sử dụng sklearn, Acc: ',acc_score(y_test, y_hat4))\n",
        "\n",
        "    kfolk(X_test)\n",
        "    print(\"Thực hiện huấn luyện mô hình với cross_validate;\")\n",
        "    crossvalidate(X_test,y_test)\n",
        "    print(\"\\nhiện huấn luyện mô hình với cross_val_score\")\n",
        "    cross_valscore(X_test,y_test)\n",
        "\n",
        "    y_pred=cross_valpredict1(X_test,y_test)\n",
        "    print(\"\\nSử dụng cross_val_predict để dự đoán.\",y_pred)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igxajHb9dUBP",
        "outputId": "0742e695-ced4-4eb4-f135-a66836dcac20"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.5067801656080071\n",
            "-0.3270580397857181\n",
            "Trong so w toi uu la L-BFGS-B:  [ 2.83260864 16.76544116 21.14450233]\n",
            "tGia tri Loss toi uu:  0.18479478833312113\n",
            "tTrong so w toi uu la Powell:  [ 2.83085511 16.75294021 21.12525394]\n",
            "tGia tri Loss toi uu:  0.18479487372203188\n",
            "Sử dụng sklearn, Acc:  0.9\n",
            "....\n",
            "mô hình LogisticRegression\n",
            "trong so toi uu  [[-1.18859659e-05  2.38821619e+00  2.56233703e+00]]\n",
            "Sử dụng sklearn, Acc:  0.8666666666666667\n",
            "Thực hiện huấn luyện mô hình với cross_validate;\n",
            "[-0.20661157 -0.0137741  -0.01890359 -0.50409578]\n",
            "\n",
            "hiện huấn luyện mô hình với cross_val_score\n",
            "[-0.17361111 -0.78125    -0.3125      0.          0.        ]\n",
            "\n",
            "Sử dụng cross_val_predict để dự đoán. [0.72727273 0.72727273 0.72727273 0.72727273 0.72727273 0.72727273\n",
            " 0.72727273 0.72727273 0.68181818 0.68181818 0.68181818 0.68181818\n",
            " 0.68181818 0.68181818 0.68181818 0.68181818 0.65217391 0.65217391\n",
            " 0.65217391 0.65217391 0.65217391 0.65217391 0.65217391 0.60869565\n",
            " 0.60869565 0.60869565 0.60869565 0.60869565 0.60869565 0.60869565]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    }
  ]
}